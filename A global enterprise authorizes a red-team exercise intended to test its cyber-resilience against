1. To what extent is informed consent achievable or necessary in realistic security testing that relies on deception and AI-driven personalization?

Achieving explicit informed consent from every individual employee in a red-team exercise is often fundamentally incompatible with the primary objective of testing realistic reactions to sophisticated threats. If employees are fully aware that a specific communication is a test at the moment they receive it, their behavioral response will inevitably be biased, rendering the data regarding the organization's cyber-resilience inaccurate and effectively useless. However, the necessity of consent is not binary; rather, it exists on a spectrum that legal and ethical frameworks must navigate carefully, especially when AI tools are used to create deeply personal pretexts. In this specific case, the organization relied on a model of "implied consent" derived from employment contracts and general IT acceptable use policies, which typically reserve the right for the employer to monitor systems and conduct security assessments. Yet, reliance on broad policy language becomes ethically precarious when the testing methodology shifts from generic spam to AI-driven, highly personalized phishing that leverages private or semi-private context.

The introduction of AI necessitates a re-evaluation of what employees have actually consented to; they may have agreed to standard security drills, but they likely did not anticipate a simulation that mimics the psychological manipulation of a targeted state-actor attack. Therefore, while *individual, real-time* informed consent is not achievable for the test to work, a foundational "broad consent" must be established through clear, transparent communication during onboarding or annual training that such realistic simulations are a possibility. Without this foundational transparency, the organization risks violating the "psychological contract" between employer and employee, which can be far more damaging to retention and culture than the technical insights gained are valuable. Furthermore, in jurisdictions with strict data protection laws like the GDPR in Europe, the lack of transparency regarding how personal data is processed to generate these AI attacks could constitute a legal violation, regardless of the security intent. Consequently, the necessity of consent shifts from requesting permission for a specific event to ensuring a collective understanding that the organization employs advanced, deceptive adversarial simulations as a condition of defense.

To bridge this gap, organizations often utilize a "proxy consent" model, where a diverse oversight committee—including HR and legal representatives—reviews the pretext to ensure it does not cross into harassment or abuse, effectively consenting on behalf of the workforce. Nevertheless, the distress reported by employees in this case suggests that the "proxy" failed to anticipate the emotional impact of AI-driven realism, highlighting that legal authorization does not equate to ethical license. Ultimately, while full informed consent is operationally impossible, the organization is ethically obligated to provide a robust debriefing process that essentially seeks "retroactive consent" by explaining the purpose and value of the exercise immediately after the fact.

### 2. How do emerging AI capabilities alter the ethical risk profile of traditional red-team practices?

Emerging AI capabilities fundamentally alter the ethical risk profile of red teaming by shifting the scale, speed, and intimacy of social engineering attacks from a manual, resource-intensive process to an automated, high-fidelity bombardment. In traditional red teaming, crafting a hyper-personalized "spear-phishing" email required significant human research and time, naturally limiting the number of employees who could be targeted with such intensity. AI removes this bottleneck, allowing the red team to generate thousands of unique, context-aware messages that mimic the writing style of trusted colleagues or the specific urgency of a manager, as seen in this case. This democratization of sophistication means that the psychological pressure previously reserved for high-value executives is now applied to general staff, who may be less equipped to handle such targeted manipulation.

Furthermore, AI introduces the risk of "unintended escalation" or "hallucination," where the generative model might create pretexts that violate the established Rules of Engagement (RoE) by inventing distinct sensitive scenarios—such as legal threats, health crises, or family emergencies—that human operators did not explicitly authorize. The use of AI also raises profound privacy concerns regarding the "training data" used to generate these attacks; if the AI is fed internal emails to learn the voice of trusted contacts, the organization is essentially weaponizing its own employees' private communications against them. This creates a scenario where the simulation of a breach feels indistinguishable from a real invasion of privacy, potentially causing genuine trauma rather than a learning moment.

The ethical profile is further complicated by the "black box" nature of some AI tools, which makes it difficult for the red team to guarantee that the generated content will remain within safe boundaries, unlike a static email template which is reviewed and approved line-by-line. Additionally, the efficacy of AI-driven deepfakes (audio or video) introduces a level of deception that exploits biological trust markers—like recognizing a voice—which can erode the fundamental interpersonal trust required for a team to function effectively. When employees can no longer trust that a digital communication from a known colleague is authentic, the operational friction within the company increases, potentially causing more damage than the simulated attack itself. Consequently, the ethical risk is no longer just about "fooling" an employee, but about permanently degrading the high-trust culture that allows the organization to operate.

### 3. Can proportionality be meaningfully assessed when simulated harm primarily affects employees rather than technical systems?

Assessing proportionality in the context of human-centric security testing is significantly more complex than in technical testing because psychological harm is subjective, invisible, and often delayed, unlike a server crash or a service outage which is immediate and measurable. In this case, the authorization focused on technical objectives—testing data exfiltration paths and encryption simulations—but the actual "collateral damage" manifested as employee distress, a metric that traditional risk assessments often fail to quantify. True proportionality requires balancing the imperative of national or corporate security against the specific dignity and mental well-being of the workforce, a calculus that becomes skewed when the simulation is indistinguishable from a career-ending crisis.

If an employee believes they have lost critical data or compromised the company due to a simulated ransomware attack, the resulting anxiety, panic, and potential reputational fear constitute real harm, even if no files were actually encrypted. The organization must ask whether the lesson learned—"don't click this link"—is valuable enough to justify causing an employee to experience a genuine "fight or flight" physiological response. When AI is used to maximize the convincing nature of the threat, the simulation moves beyond testing *adherence to policy* and begins testing *emotional resilience*, which may not be a fair or relevant metric for many job roles.

Proportionality also fails when the simulation leverages power dynamics that employees cannot reasonably question, such as an urgent directive from the CEO or Legal department, effectively trapping them in a "damned if you do, damned if you don't" scenario. In such cases, the test measures obedience rather than security awareness, and the distress caused is disproportionate because the employee was placed in an impossible situation. Furthermore, across different jurisdictions, the definition of "harm" varies; what is considered a "stressful day" in one culture might be viewed as "constructive dismissal" or "workplace harassment" in another. Therefore, meaningful assessment of proportionality must include "psychological safety" as a critical infrastructure component, just as vital as network bandwidth. If the exercise degrades the psychological safety of the workforce to the point where they are afraid to open legitimate emails, the cost has exceeded the benefit, and the exercise is disproportionately harmful.

### 4. What governance structures are required to balance realism, legality, and psychological safety in AI-enabled security testing?

To safely conduct AI-enabled security testing, organizations require a multi-disciplinary governance structure that extends far beyond the traditional approval chain of the CISO and General Counsel. A dedicated "Ethical Oversight Committee" should be established, comprising representatives from Human Resources, Legal, Privacy, Communications, and potentially an external ethics advisor, to review the psychological implications of proposed scenarios. This committee must be empowered to veto or alter scenarios that, while technically legal, pose an unacceptable risk to employee well-being or corporate culture.

Crucially, the governance framework must establish strict, granular "Rules of Engagement" (RoE) specifically for AI usage, explicitly defining off-limits topics (e.g., health, family, payroll, legal investigations) and limiting the "temperature" or aggression level of AI-generated content. These RoE must also mandate a "kill switch" mechanism—a rapid, pre-planned communication protocol that allows the organization to instantly declare "this is a drill" if real-world panic begins to spiral out of control. Additionally, governance must dictate data handling procedures for the AI tools themselves, ensuring that employee data used to train or prompt the phishing models is siloed, anonymized, and deleted immediately after the exercise to prevent privacy leaks.

There must also be a formal "safety valve" process where employees can report suspicious activity without fear of retribution, and where the response team can discreetly verify if an event is part of the simulation. Post-exercise governance is equally vital; it should mandate "blame-free" debriefing sessions that focus on systemic vulnerabilities rather than individual failures, ensuring that the narrative remains educational rather than punitive. In multi-jurisdictional organizations, the governance structure must also include regional compliance officers who can localize the exercise parameters to ensure they align with local labor laws and cultural norms regarding monitoring and privacy. Finally, executive leadership must remain accountable not just for the *results* of the test, but for the *conduct* of the test, ensuring that the pursuit of security does not become a justification for internal abuse.

### 5. Under what conditions should security professionals refuse or constrain authorized activities on ethical grounds?

Security professionals retain a professional and ethical duty that transcends their employment contract, obligating them to refuse or constrain activities when they believe an exercise will cause foreseeable, disproportionate, or irreversible harm to individuals. Even with executive and legal authorization, a security professional should refuse to execute a scenario if it exploits personal tragedies, medical conditions, or financial vulnerabilities of employees, as these tactics cross the line from professional testing into personal harassment. If a planned AI-driven pretext involves impersonating family members or government authorities in a way that could incite genuine terror or legal panic, the professional is ethically bound to push back.

Refusal is also warranted if the security professional identifies that the organization lacks the necessary support structures—such as a responsive helpdesk or mental health resources—to manage the fallout of a high-stress simulation. Furthermore, if the exercise is designed in a way that makes failure inevitable for the sole purpose of justifying budget increases or punishing staff, it violates the ethical principle of integrity and should be opposed. Professionals must also constrain activities when the technical scope is too broad or poorly defined, specifically when using autonomous AI agents that might act unpredictably or "escape" the test environment to affect production systems or external partners.

In multi-jurisdictional contexts, a security professional must refuse to carry out actions in a region where they believe the activity would violate local privacy or labor laws, regardless of whether corporate headquarters in a different country has approved it. The "Nuremberg defense"—claiming one was simply following orders—is generally not accepted in professional certification codes of ethics (such as those from ISC2 or ISACA), meaning the individual practitioner is personally responsible for their professional conduct. Therefore, if the "tone at the top" ignores repeated warnings about employee distress or privacy violations, the security professional has a duty to formally document their objection and, in extreme cases, refuse to participate to preserve their professional standing. Ultimately, the goal of security is to protect the organization, which includes its people; any activity that attacks the people to the point of breaking them is antithetical to the profession's core mission.
